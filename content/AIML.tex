
\section{Artificial Intelligence / Machine Learning}

\subsection{Artificial Intelligence}
There are some goal states and one initial state. The objective is to find the goal state that is closer (with the shortest path to the root/ with the least search cost). Is given as the solution the steps necessary to perform that path.

To keep the formulation as general as possible, abstractions are required, keeping in mind that they will need a correspondence when applied to a real problem.

Example of the application to a problem: a vacuum cleaner that needs to clean every square. In this case, only 2 squares are presented, no localization sensors are present, only "rubbish" sensors that tell if the current square is dirty or not.

Therefore:
\begin{itemize}
    \item \ul{States}: $<r, d_1, d_2>$ where $r$ is the robot position, $d_1$ and $d_2$ are binary, representing the existence of dirt in each of the rooms.
    \item \ul{Operators/Actions}: L (go left), R (go right) or S (suck dirt)
    \item \ul{Goal Test}: $d_1$ = False and $d_2$ = False. (($d_1$ nor $d_2$) == 1)
    \item \ul{Initial State}: $<r, d_1, d_2> = <1, T, T>$ is at square 1, and squares 1 and 2 are dirty
    \item \ul{Step Cost}: the description of each action cost. In this case, 1 for each one.
\end{itemize}


\quickimagesidebyside{AIML/cap3-001.png}{1}{AIML/cap3-002.png}{.8}

\quickimagesidebyside{AIML/cap3-003.png}{1}{AIML/cap3-004.png}{1}

\quickimagesidebyside{AIML/cap3-005.png}{1}{AIML/cap3-007.png}{1}




As you can see, every problem is very well defined in terms of the start, the end, the possible moves and what makes a solution better than other. Therefore, the conditions to put the computer thinking how to get from the possible steps to the best solution are assembled. One way the computer should be able to get to the solution is by exploring every move combination and check the state it ended up with.


Given the initial state, use the operators to generate successor states.

\quickimage{AIML/cap3-008.png}{.6}

Then a choice of which is more "profitable" or more likely to lead to a goal state needs to be made and this cycle continues until the arrival at a goal state.

\quickimage{AIML/cap3-009.png}{.6}

In this case, the solution would be $\{S, R, S\}$.




However, before diving directly in algorithms, is important to note the characteristics of the Environment and of the Agent. Based on these we'll be able to perform much better choosing the search algorithm that is best for us.

\subsubsection{Environment} %FORMULATE THIS SECTION BETTER!!!!!!!!!!!!!!!!!!

Observability: how much can the agent know about the environment.

An environment is fully observable if the agent can see everything. Or partial observable:
- part of the state is occulted and you simply can't 



Deterministic: the effect of the actions are predictable
If I move one queen from one place to the other, I can predict the effects, what is attacking what, etc\dots
Instead of Deterministic, it can be stochastic, where the outcomes are functions of probability functions. Therefore, you can't be 100\% sure of the outcome. Therefore, you can use probability to make choices\dots


Neither the environment nor the agent performance change while the agent is deliberating. --> Static.

In a dynamic environment, the agent performance can change with time. 

Semi-dynamic means that the world is static, but the performance is changing. 
ex: turn game where the game doesn't change while you are thinking but the more you take, the less point you get.

Continuous or discrete


Sequential or episodic. Episodic means that one episode doesn't influence the next one. It's very related with causality. In episodic environments, there's no influence in consequent problems.

E.g. one game of chess is sequential, but different games are episodic.


The outcomes for all actions are given. --> Known environment.



In a case where the less time you spend thinking before answering, 


Knowing these is very important because it allows us to best choose the shelf of methods from which we take our algorithms from. Some are best from some things and some can only be used in certain situations as well.


Internally to the agent, you have a way of representing of the world. (Internal representation)

Example when you don't have an internal representation: random vacuum cleaning robots don't know anything about the environment, they just rotate randomly when they see an object.

%CHECK: how many iterations are needed depending on uniform function of the angle. Maybe if it rotates only between pi/2 and pi, it will clean everything quicker.


\quickimage{AIML/cap2-001.png}{.5}
\quickimage{AIML/cap2-002.png}{.5}

\subsubsection{Agent}

Model-based: typically, when you have partial observability.

Goal-based: when you aim for a goal. You have states and actions and can search for the goal.

Utility-based: are very similar to goal-based but are a bit more, not all goals are the same. There a preference between goals. 
Utility Theory handles how to translate preferences to numbers. 







\subsubsection{Search Problems}

In essence, is necessary some \bb{Search Terminology} to refer to certain things:

\quickimage{AIML/cap3-010.png}{.5}

General algorithm shape, starting with $open\_list = \{initial_node\}$, iterate over:
\begin{enumerate}
    \item Select a node from the open\_list;
    \item Check if it's a goal node (in case it satisfies the goal test). If yes, return solution by backing up to the root;
    \item If not, remote it from the open\_list, expand it with the successor function and insert the nodes that come from there to the open\_list.
\end{enumerate}

Different selection criteria leads to a variety of search methods.

If it's tree based, then no nodes will be repeated and it's not necessary to have a list of the already visited nodes. In a graph however, is needed to have a list of these, or else a cycle is possible.

To evaluate the algorithm, many parameters may be enumerated:

\quickimage{AIML/cap3-011.png}{.5}

\vspace{.5cm}
Mentioning types of search strategies:

\quickimage{AIML/cap3-012.png}{.5}


\subsubsection{Uniformed Search Strategies}

A list of \bb{Uniformed search strategies} we'll have a deeper look to:
\begin{itemize}
    \item \bb{Breadth-first Search} - Select \ul{earliest} expanded node first - uses a FIFO queue (First In, First out). This leads to opening every node at the safe depth first before moving the deeper nodes. 
    
    \quickimage{AIML/cap3-013.png}{.5}

    Because order of depth is followed and every node checked, this search strategy is \ul{Complete} and \ul{Optimal} (if the path cost increases - or at least doesn't decreases - with depth).
    Being \bb{d} the depth of the solution and \bb{b} the branching factor(max number of successors of a node.) then in the worst case, the total number of nodes generated is: $1 + b + b^2 + b^3 + \dots + b^d$

    Time Complexity - O($b^d$)
    Space Complexity - O(sum no of nodes) = O($b^d$)

    \quickimage{AIML/cap3-014.png}{.5}

    Note that it makes a difference if you test the node before or after expanding it. If it's tested before, there's no need of expanding the node. If the test is made only after the expansion, the complexities grows to O($b^{d+1}$). 


    \item \bb{Uniform-cost Search} - expands the node that has the smallest cost from the root to it. 

    \quickimage{AIML/cap3-015.png}{.5}

    Note that each action generates one possible state from the state where the robot was previously.

    This search strategy is only complete and optimal if the step costs are strictly positive. Else, it can give several steps that cost nothing to places far away from the solution.

    \item \bb{Depth-first Search} - Exactly as the name suggests, goes until the deepest node first, and only then looks at the other nodes at the first level. Therefore, it can be very inefficient on a large or infinite tree. Open the last node added to the list (LIFO- Last In, First Out).


    \quickimage{AIML/cap3-016.png}{.5}

    
    \item \bb{Backtrack Search} - a variation of depth-first, but expands one node at a time, only stores in memory that only node and the expansion is made by modifying the node, while backtrack is nothing more than undoing the modification..
    
    It's not complete nor it is optimal... but saves a lot of memory.

    \item \bb{Depth-limited search} - another variation of depth-first where limiting the search tree to a depth L, is possible to contain the inefficiency. It's complete if the depth of the solution if smaller than L but still not optimal. 
    
    Time complexity: O($b^L$)
    Space complexity: O($b \times L$)

    \item \bb{Iterative deepening depth-first search} - A variation of the previous one. In this one, the idea will be to run depth-limited search for an increasing L. Run for L=1, L=2, \dots
    
    This way, it is complete and it's optimal (if the path cost is a non-decreasing function of depth).

    Time complexity: O($b^d$)
    Space complexity: O($b \times d$)

    \quickimage{AIML/cap3-017.png}{.5}


    \item \bb{Bidirectional Search} - Search both from initial node and from the gold node. Note however that can only be used when a goal node is known and when the parent nodes can be computed given its child (through the sets of available actions). It's complete (if breath-first in both directions) and Optimal, if the step costs are equal.
    
    Time and space complexity: O($b^{d/2}$)

    \quickimage{AIML/cap3-018.png}{1}
\end{itemize}


A summary of the above analysis:
    
\quickimage{AIML/cap3-019.png}{.7}

A \bb{General Search Algorithm} can be formulated in the following way:

\quickimage{AIML/cap3-020.png}{.6}



Finally, there are problem dependent and problem independent details. The search strategies are problem independent, but how we define the actions, states, ect\dots is problem dependent. And the way we see and think about the problem can be highly related to the way we represent it. 

An incredibly well formulated example is the Mutilated Chess Board problem. If we take just the squares of the corners of a chess board, can we still fill the whole board with dominos (each domino takes 2 squares). It becomes slightly harder! If you keep removing squares it gets increasingly harder to figure out by head. 

\quickimage{AIML/cap2-003.png}{.6}

However, if you represent the chess board as the remaining black and white pieces and notice that a domino piece must always cover one black and one white squares, then by taking those two corners then 30 black and 32 white squares will be remaining making it impossible to fill with dominos. As a matter of fact, accordingly with Gomory's Theorem is also possible to say that if 2 square of opposite colours are removed then is always possible to fill the board with dominos! More in: \href{https://en.wikipedia.org/wiki/Mutilated_chessboard_problem}{\ul{Mutilated chessboard problem}}

The bottom line is that the representation (problem dependent details) matters.


With this is mind, consider the following example:

Initial state with:
bedroom(3), living room(2), kitchen(3), hall(2), truck(0)



\begin{enumerate}[a)]
    \item State: $<pos, n_{bedroom}, n_{living}, n_{kitchen}, n_{hall}, n_{truck}>$
    \item Initial State: $<truck, 3,2,3,2,0>$
    \item $m$ for move, $p$ for push $<m_N, m_S, m_E, m_W, p_N, p_W, p_E, p_W >$. However, is only possible to push in directions where there are rooms (as for walking) and when there are boxes in the current room we are located in.
    \item A goal condition could be $n_{truck} = 10$. Another could be the sum of all rooms to be 0. But the first one is more elegant and also seems more general... We don't want to throw boxes out of the window.
\end{enumerate}






Sometimes, just finding the solution is enough. Some times, there's a best solution.


The heuristic (the only difference between informed and uninformed search) is a function of a state that gives us the appreciation we have for that state - how good that state is. If we want the best solution, we must have the heuristic function is key. If you just want a solution, that function can be much simpler or even nonexistent.







\subsubsection{Informed Search Strategies}
A problem-solving agent is a goal-based agent that acts on the environment,
leading him to go through a series of states in order to achieve the desired goal


\subsection{Supervised Learning}





\subsubsection*{Extrema Conditions and Hessian Matrix}

Videos: \href{https://www.youtube.com/watch?v=nRJM4mY-Pq0&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=87}{\ul{87 - Warm up to the second partial derivative test}} to\dots \href{https://www.youtube.com/watch?v=sJo7D74PAak&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=89}{\ul{89 - Second partial derivative test intuition}}




This sub (...) sub section aims to explain why the conditions imposed on the Hessian matrix - the second derivative matrix - correspond to imposing in 2D what we know already.

In practice, in 2D, to find a extrema we need to guarantee that the first derivative is zero, which in N dimensions is correspondent to guaranteeing that the gradient is zero in that point, because the gradient is nothing more than all partial derivatives in that point. Therefore, setting $\triangledown f = 0$ means that, in that point, the function is not increasing or decreasing in any of the N directions. So the first derivative makes sense.

However, when we go to the second derivative, the meanings get a bit more complicated.

In 2D, if the second derivative was 0, it was probably (not certainly) a saddle point, if it was $> 0$ or $< 0$ it was, respectively, a local minimum or maximum.

The conditions we should impose in 3D is to have a positive (for finding a minimum) or negative (for finding a maximum) definite Hessian Matrix. While second derivatives in order to just one variable is possible to attribute a sense to it, why do the cross derivatives play a role as well? And, why do they mean really?

Well, first the explanation on why it is needed: there are functions that across multiple dimensions still show that it is an extrema but then there's an inflexion along directions that are not along the axis. So, checking the axis is not enough. Why checking the cross partial derivatives makes it enough?

\ul{The Second Derivative Test}

\begin{equation}
    f_{xx}(x_o, y_o) f_{yy}(x_o, y_o) - f_{xy}(x_o, y_o)^2 \gtrless 0
\end{equation}

If it is greater than 0, we have a maximum or a minimum and have to check the value of $f_{xx}$ or $f_{yy}$ to be sure. If it is less than 0, we have a saddle point. If it equals 0, then we don't know if it is a saddle point, but it is not a min or max therefore, at least for now, we certainly don't care.

Cross or Mixed partial derivatives can be switched? Yes if the function is $C^2$. 
(Boring to prove theorem called: Schwarz' Theorem)

Therefore, we just need to compute one of the cross derivatives.


Also this works because the second derivative test is nothing more than the determinant of the Hessian matrix. The determinant is the product of every eigenvalue of that matrix, therefore it can only be positive if they are both positive or both negative, in which cases there is, respectively, a minimum or a maximum. 

But why do eigenvalues tell us this? Because they tell us how the eigenvectors are scaled! And the eigenvectors of such matrix will be the greatest and the least curvatures. Therefore, they either have the same signal / are scaled the same amount, or 



Some other links that helped with this:
\begin{itemize}
    \item \href{http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MORSE/diffgeom.pdf}{\ul{Differential Geometry}}
    \item \href{http://math.mit.edu/classes/18.013A/HTML/chapter11/section02.html}{\ul{Criterior for critical points - Maximum, Minimum or Saddle?}}
    \item \href{https://www.adelaide.edu.au/mathslearning/play/seminars/evalue-magic-tricks-handout.pdf}{David Butler - Facts about Eigenvalues}
\end{itemize}





\subsubsection{Neural Networks - BackPropagation}

Two of the most useful websites to check while trying to demonstrate the backpropagation algorithm:
\begin{itemize}
    \item \href{https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60}{\ul{all backpropagation derivatives}}
    \item \href{https://stats.stackexchange.com/questions/94387/how-to-derive-errors-in-neural-network-with-the-backpropagation-algorithm}{\ul{Error (deltas) derivation for backpropagation in neural networks}}
    \item \href{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{3Blue1Brown Neural Networks} - Specially the last one, on backpropagation.
    
\end{itemize}



Why kernels are important? They facilitate a lot the mapping of features to higher dimensions!

\href{https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d}{\ul{Why kernels?}}



\subsection{Unsupervised Learning}

\subsubsection{Lagrange Multipliers}

Perfect Explanation why it works:

\href{https://www.quora.com/Why-does-the-method-of-Lagrange-multipliers-work-for-optimization-in-multivariable-calculus-Why-exactly-given-a-function-f-x-y-and-a-constraint-g-x-y-c-can-we-set-the-gradients-of-the-functions-to-be-multiples-of-each-other}{\ul{Quora - Intuition on Langrange Multipliers}}

And a video showing exactly how it's done:

\href{https://www.youtube.com/watch?v=yuqB-d5MjZA}{\ul{Khan Academy - Lagrange Multipliers Example}}



\subsection{Reinforcement Learning and Decision Making}


An agent to make the wisest decision in its situation has to have knowledge on what state he is in, how the environment will evolve, how it will be like if he performs a certain action (taking into account stochastic environments) and a utility function to  know how much that state contributes to its happiness.


