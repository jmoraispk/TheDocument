
\section{Artificial Intelligence / Machine Learning}

\subsection{Artificial Intelligence}
There are some goal states and one initial state. The objective is to find the goal state that is closer (with the shortest path to the root/ with the least search cost). Is given as the solution the steps necessary to perform that path.

To keep the formulation as general as possible, abstractions are required, keeping in mind that they will need a correspondence when applied to a real problem.

Example of the application to a problem: a vacuum cleaner that needs to clean every square. In this case, only 2 squares are presented, no localization sensors are present, only "rubbish" sensors that tell if the current square is dirty or not.

Therefore:
\begin{itemize}
    \item \ul{States}: $<r, d_1, d_2>$ where $r$ is the robot position, $d_1$ and $d_2$ are binary, representing the existence of dirt in each of the rooms.
    \item \ul{Operators/Actions}: L (go left), R (go right) or S (suck dirt)
    \item \ul{Goal Test}: $d_1$ = False and $d_2$ = False. (($d_1$ nor $d_2$) == 1)
    \item \ul{Initial State}: $<r, d_1, d_2> = <1, T, T>$ is at square 1, and squares 1 and 2 are dirty
    \item \ul{Step Cost}: the description of each action cost. In this case, 1 for each one.
\end{itemize}


\quickimagesidebyside{AIML/cap3-001.png}{1}{AIML/cap3-002.png}{.8}

\quickimagesidebyside{AIML/cap3-003.png}{1}{AIML/cap3-004.png}{1}

\quickimagesidebyside{AIML/cap3-005.png}{1}{AIML/cap3-007.png}{1}




As you can see, every problem is very well defined in terms of the start, the end, the possible moves and what makes a solution better than other. Therefore, the conditions to put the computer thinking how to get from the possible steps to the best solution are assembled. One way the computer should be able to get to the solution is by exploring every move combination and check the state it ended up with.


Given the initial state, use the operators to generate successor states.

\quickimage{AIML/cap3-008.png}{.6}

Then a choice of which is more "profitable" or more likely to lead to a goal state needs to be made and this cycle continues until the arrival at a goal state.

\quickimage{AIML/cap3-009.png}{.6}

In this case, the solution would be $\{S, R, S\}$.




However, before diving directly in algorithms, is important to note the characteristics of the Environment and of the Agent. Based on these we'll be able to perform much better choosing the search algorithm that is best for us.

\subsubsection{Environment} %FORMULATE THIS SECTION BETTER!!!!!!!!!!!!!!!!!!

Observability: how much can the agent know about the environment.

An environment is fully observable if the agent can see everything. Or partial observable:
- part of the state is occulted and you simply can't 



Deterministic: the effect of the actions are predictable
If I move one queen from one place to the other, I can predict the effects, what is attacking what, etc\dots
Instead of Deterministic, it can be stochastic, where the outcomes are functions of probability functions. Therefore, you can't be 100\% sure of the outcome. Therefore, you can use probability to make choices\dots


Neither the environment nor the agent performance change while the agent is deliberating. --> Static.

In a dynamic environment, the agent performance can change with time. 

Semi-dynamic means that the world is static, but the performance is changing. 
ex: turn game where the game doesn't change while you are thinking but the more you take, the less point you get.

Continuous or discrete


Sequential or episodic. Episodic means that one episode doesn't influence the next one. It's very related with causality. In episodic environments, there's no influence in consequent problems.

E.g. one game of chess is sequential, but different games are episodic.


The outcomes for all actions are given. --> Known environment.



In a case where the less time you spend thinking before answering, 


Knowing these is very important because it allows us to best choose the shelf of methods from which we take our algorithms from. Some are best from some things and some can only be used in certain situations as well.


Internally to the agent, you have a way of representing of the world. (Internal representation)

Example when you don't have an internal representation: random vacuum cleaning robots don't know anything about the environment, they just rotate randomly when they see an object.

%CHECK: how many iterations are needed depending on uniform function of the angle. Maybe if it rotates only between pi/2 and pi, it will clean everything quicker.


\quickimage{AIML/cap2-001.png}{.5}
\quickimage{AIML/cap2-002.png}{.5}

\subsubsection{Agent}

Model-based: typically, when you have partial observability.

Goal-based: when you aim for a goal. You have states and actions and can search for the goal.

Utility-based: are very similar to goal-based but are a bit more, not all goals are the same. There a preference between goals. 
Utility Theory handles how to translate preferences to numbers. 







\subsubsection{Search Problems}

In essence, is necessary some \bb{Search Terminology} to refer to certain things:

\quickimage{AIML/cap3-010.png}{.5}

General algorithm shape, starting with $open\_list = \{initial_node\}$, iterate over:
\begin{enumerate}
    \item Select a node from the open\_list;
    \item Check if it's a goal node (in case it satisfies the goal test). If yes, return solution by backing up to the root;
    \item If not, remote it from the open\_list, expand it with the successor function and insert the nodes that come from there to the open\_list.
\end{enumerate}

Different selection criteria leads to a variety of search methods.

If it's tree based, then no nodes will be repeated and it's not necessary to have a list of the already visited nodes. In a graph however, is needed to have a list of these, or else a cycle is possible.

To evaluate the algorithm, many parameters may be enumerated:

\quickimage{AIML/cap3-011.png}{.5}

\vspace{.5cm}
Mentioning types of search strategies:

\quickimage{AIML/cap3-012.png}{.5}


\subsubsection{Uniformed Search Strategies}

A list of \bb{Uniformed search strategies} we'll have a deeper look to:
\begin{itemize}
    \item \bb{Breadth-first Search} - Select \ul{earliest} expanded node first - uses a FIFO queue (First In, First out). This leads to opening every node at the safe depth first before moving the deeper nodes. 
    
    \quickimage{AIML/cap3-013.png}{.5}

    Because order of depth is followed and every node checked, this search strategy is \ul{Complete} and \ul{Optimal} (if the path cost increases - or at least doesn't decreases - with depth).
    Being \bb{d} the depth of the solution and \bb{b} the branching factor(max number of successors of a node.) then in the worst case, the total number of nodes generated is: $1 + b + b^2 + b^3 + \dots + b^d$

    Time Complexity - O($b^d$)
    Space Complexity - O(sum no of nodes) = O($b^d$)

    \quickimage{AIML/cap3-014.png}{.5}

    Note that it makes a difference if you test the node before or after expanding it. If it's tested before, there's no need of expanding the node. If the test is made only after the expansion, the complexities grows to O($b^{d+1}$). 


    \item \bb{Uniform-cost Search} - expands the node that has the smallest cost from the root to it. 

    \quickimage{AIML/cap3-015.png}{.5}

    Note that each action generates one possible state from the state where the robot was previously.

    This search strategy is only complete and optimal if the step costs are strictly positive. Else, it can give several steps that cost nothing to places far away from the solution.

    \item \bb{Depth-first Search} - Exactly as the name suggests, goes until the deepest node first, and only then looks at the other nodes at the first level. Therefore, it can be very inefficient on a large or infinite tree. Open the last node added to the list (LIFO- Last In, First Out).


    \quickimage{AIML/cap3-016.png}{.5}

    
    \item \bb{Backtrack Search} - a variation of depth-first, but expands one node at a time, only stores in memory that only node and the expansion is made by modifying the node, while backtrack is nothing more than undoing the modification..
    
    It's not complete nor it is optimal... but saves a lot of memory.

    \item \bb{Depth-limited search} - another variation of depth-first where limiting the search tree to a depth L, is possible to contain the inefficiency. It's complete if the depth of the solution if smaller than L but still not optimal. 
    
    Time complexity: O($b^L$)
    Space complexity: O($b \times L$)

    \item \bb{Iterative deepening depth-first search} - A variation of the previous one. In this one, the idea will be to run depth-limited search for an increasing L. Run for L=1, L=2, \dots
    
    This way, it is complete and it's optimal (if the path cost is a non-decreasing function of depth).

    Time complexity: O($b^d$)
    Space complexity: O($b \times d$)

    \quickimage{AIML/cap3-017.png}{.5}


    \item \bb{Bidirectional Search} - Search both from initial node and from the gold node. Note however that can only be used when a goal node is known and when the parent nodes can be computed given its child (through the sets of available actions). It's complete (if breath-first in both directions) and Optimal, if the step costs are equal.
    
    Time and space complexity: O($b^{d/2}$)

    \quickimage{AIML/cap3-018.png}{1}
\end{itemize}


A summary of the above analysis:
    
\quickimage{AIML/cap3-019.png}{.7}

A \bb{General Search Algorithm} can be formulated in the following way:

\quickimage{AIML/cap3-020.png}{.6}



Finally, there are problem dependent and problem independent details. The search strategies are problem independent, but how we define the actions, states, ect\dots is problem dependent. And the way we see and think about the problem can be highly related to the way we represent it. 

An incredibly well formulated example is the Mutilated Chess Board problem. If we take just the squares of the corners of a chess board, can we still fill the whole board with dominos (each domino takes 2 squares). It becomes slightly harder! If you keep removing squares it gets increasingly harder to figure out by head. 

\quickimage{AIML/cap2-003.png}{.6}

However, if you represent the chess board as the remaining black and white pieces and notice that a domino piece must always cover one black and one white squares, then by taking those two corners then 30 black and 32 white squares will be remaining making it impossible to fill with dominos. As a matter of fact, accordingly with Gomory's Theorem is also possible to say that if 2 square of opposite colours are removed then is always possible to fill the board with dominos! More in: \href{https://en.wikipedia.org/wiki/Mutilated_chessboard_problem}{\ul{Mutilated chessboard problem}}

The bottom line is that the representation (problem dependent details) matters.


With this is mind, consider the following example:

Initial state with:
bedroom(3), living room(2), kitchen(3), hall(2), truck(0)



\begin{enumerate}[a)]
    \item State: $<pos, n_{bedroom}, n_{living}, n_{kitchen}, n_{hall}, n_{truck}>$
    \item Initial State: $<truck, 3,2,3,2,0>$
    \item $m$ for move, $p$ for push $<m_N, m_S, m_E, m_W, p_N, p_W, p_E, p_W >$. However, is only possible to push in directions where there are rooms (as for walking) and when there are boxes in the current room we are located in.
    \item A goal condition could be $n_{truck} = 10$. Another could be the sum of all rooms to be 0. But the first one is more elegant and also seems more general... We don't want to throw boxes out of the window.
\end{enumerate}






Sometimes, just finding the solution is enough. Some times, there's a best solution.


The heuristic (the only difference between informed and uninformed search) is a function of a state that gives us the appreciation we have for that state - how good that state is. If we want the best solution, we must have the heuristic function is key. If you just want a solution, that function can be much simpler or even nonexistent.







\subsubsection{Informed Search Strategies}
A problem-solving agent is a goal-based agent that acts on the environment,
leading him to go through a series of states in order to achieve the desired goal


\bb{In this course, we only study the single-state problems}


\subsection{Supervised Learning}

Supervised learning concerns the problems where the objective is to predict something based on previous data. The counterpart Unsupervised Learning tries to find patterns in unlabelled data.

More generally, the dataset for supervised learning problems consists on a feature vector $\mathbf{x}$ and a output vector $\mathbf{y}$. Regarding unsupervised learning, everything is features / data.


\subsubsection{Regression Problems - Least Squares}
These are the two most commons types of problems. Probably every supervised learning problem can be \ii{classified} as one of these.

Regression is when the output should be continuous, classification when the output should be in discrete classes. 

About the first, a measure to minimize is the difference between our prediction to the value we want to achieve. The Sum of the Square Errors (SSE) is very standard cost function to minimize.

The function that is required to minimize is loss/cost/risk function. Nomenclature wise is a problem... Therefore, the following letters/terms can be will be used interchangibly: $L$ (Loss Function) or $J$ (more used when the weights or coefficients are $\theta$) or $R$ (Risk Function):
\begin{equation}
    R = \frac{1}{2m} \sum_{i = 1}^m \left(y^{(i)} - \hat{y}^{(i)}\right)^2
\end{equation}

Where $\hat{y}$ is our prediction or estimation of the true value of $y$ and \bb{m} is the number of training samples we have. Therefore $y^(i)$ constitutes the outcome of sample $i$.

One prediction can be made with:
\begin{equation}
    \hat{y} = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
\end{equation}
Where \bb{n} is the number of features we are using. Note that features and data points are different things. We can have data regarding only one measure but take the square and the cube of the measure multiplied to different coefficients in order to try to better estimate the function.

$\beta$'s are usually used in regressions while in neural networks letters like $\theta$ or $w$ are more common.


One intuition that is important to have is that \bb{the more we increase the features, the more likely it is that we end up overfitting our training set and loosing generalization capabilities for the actual test data}. This is why it is important to divide all the available data in sets, the model will be trained to guess the training samples, not samples that it never saw before.
\begin{itemize}
    \item One set for training and one set for testing the prediction capabilities;
    \item One training set, one validation set and one test set.
\end{itemize}

The last option is meant to validate the model. The model is trained with the training set, then some parameters are tuned with the validation set, namely the number of polynomial features, regularization term and step size related parameters (like momentum or adaptive step size), and the actual performance is testing in the test set. This way, we avoid optimistic measures of performance by not testing in data used for training. 

\quickimage{AIML/AA-cap1-017.png}{0.4}

The result wouldn't be very different if done with the number of iterations or number of features. In particular, it is called doing an ``early stop'' when the iteration that minimises the loss in the test set is early in the minimization process. It is useful when the model starts overfitting the data.



\subsubsection{So, how do we calculate the coefficientes}
\label{sec:calcBetas}
After having the coefficients, given any other set of data points we can already give predictions on the output.

Note that we are trying to minimize the cost/risk/loss function. The actual cost function would have to be some sort of prediction because it's impossible to know exactly how much the actual outcome will be, despite knowing  exactly what the outcome of the model will be to a certain feature vector. As opposed to the empirical risk function where the training outcome and the training predicted result are used, therefore being able to calculate the difference between each estimation and the supposed outcome. To compute the real risk, the expected value of the SSE is necessary. Also, there are continuous results so an integral is required:
\begin{equation}
    R = E[y-\hat{y}] = \int_{-\infty}^{+\infty} L(y, x) \phi(y) dx dy 
\end{equation}

Because a potential function to give us a measure on how frequent certain values are is not known, the only way is to approximate the actual error empirically, using the model with some test data. This will degenerate in the actual cost function presented before. L here is meant to denote the loss of one sample which is nothing more than the squared error.


One thing that won't happen in all problems is having an analytical and optimum solution for them. Actually, minimizing the SSE is a kind of problem is called the \bb{Least Squares}. \ul{This kind of problem is very usually used in optimisation and often a closed solution is possible.}


In this case, since we are searching for the function's minimum, the functions partial derivatives need to be zero in order to have a critical point (maximum, minimum or saddle point).

In this case, 

\begin{align*}
    \frac{\delta R}{\delta \beta_0} = -2 \sum_{i = 1}^n (y^{(i)} - \beta_0 - \beta_1 x^{(i)}) = 0 \\
    \frac{\delta R}{\delta \beta_1} = -2 \sum_{i = 1}^n (y^{(i)} - \beta_0 - \beta_1 x^{(i)})x^{(i)} = 0 \\
\end{align*}

Is possible to simplify further these equations, putting the $\beta$'s in evidence and separating sums, arriving at:

\quickimage{AIML/AA-cap1-001.png}{0.6}

Is possible to invert these equations and get the expressions for the coefficients. However there's one important factor to have in mind. Are we aiming at a minimum, maximum or something different like a saddle point? The Hessian Matrix will tell us.



\subsubsection{Extrema Conditions and Hessian Matrix}

Videos: \href{https://www.youtube.com/watch?v=nRJM4mY-Pq0&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=87}{\ul{87 - Warm up to the second partial derivative test}} to\dots \href{https://www.youtube.com/watch?v=sJo7D74PAak&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=89}{\ul{89 - Second partial derivative test intuition}}


\quickimage{AIML/AA-cap1-002.png}{0.6}

In one dimension, to find an extrema is necessary to equalise the first derivative to zero, and the second derivative must be positive - in case of a minimum - or negative - in case of a maximum. If the second derivative is zero at the critical point, then there's an inflexion point. A similar analysis must be done in ($n$+1)-D where $n$ is the number of features used in the regression.

Guaranteeing that the first derivative is zero, which in N dimensions is correspondent to guaranteeing that the gradient is zero in that point, is the first step. Setting $\triangledown f = 0$ means that, in that point, the function is not increasing or decreasing in any of the N directions. So the first derivative makes sense.

However, when we go to the second derivative, the meanings get a bit more complicated.

In 2D, if the second derivative was 0, it was certainly a saddle point, if it was $> 0$ or $< 0$ it was, respectively, a local minimum or maximum.

The conditions we should impose in 3D is to have a positive (for finding a minimum) or negative (for finding a maximum) definite Hessian Matrix. While it is possible to attribute a meaning to second derivatives in order to just one variable, being nothing more than the concavity in those 2 directions, why do the cross derivatives play a role as well? And, why do they mean really?

Well, first the explanation on why it is needed: there are functions that across multiple dimensions still show that it is an extrema but then there's an inflexion along directions that are not along the axis. So, checking the axis is not enough. Why checking the cross partial derivatives makes it enough?

\ul{The Second Derivative Test}

\begin{equation}
    f_{xx}(x_o, y_o) f_{yy}(x_o, y_o) - f_{xy}(x_o, y_o)^2 \gtrless 0
\end{equation}

If it is greater than 0, we have a maximum or a minimum and have to check the value of $f_{xx}$ or $f_{yy}$ to be sure. If it is less than 0, we have a saddle point. If it equals 0, then we don't know if it is a saddle point, but it is not a min or max therefore, at least for now, we certainly don't care.

Cross or Mixed partial derivatives can be switched? Yes if the function is $C^2$. 
(Boring to prove theorem called: Schwarz' Theorem)

Therefore, we just need to compute one of the cross derivatives.


Also this works because the second derivative test is nothing more than the determinant of the Hessian matrix. The determinant is the product of every eigenvalue of that matrix, therefore it can only be positive if they are both positive or both negative, in which cases there is, respectively, a minimum or a maximum. 

But why do eigenvalues tell us this? Because they tell us how the eigenvectors are scaled! And the eigenvectors of such matrix will be the greatest and the least curvatures. Therefore, they either have the same signal / are scaled the same amount, or 



Some other links that helped with this:
\begin{itemize}
    \item \href{http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MORSE/diffgeom.pdf}{\ul{Differential Geometry}}
    \item \href{http://math.mit.edu/classes/18.013A/HTML/chapter11/section02.html}{\ul{Criterior for critical points - Maximum, Minimum or Saddle?}}
    \item \href{https://www.adelaide.edu.au/mathslearning/play/seminars/evalue-magic-tricks-handout.pdf}{David Butler - Facts about Eigenvalues}
\end{itemize}


The two main properties of eigenvalues that allow us to quickly calculate them from the Hessian matrix (specially if it is 2x2) are:

\begin{equation}
    \text{tr}(A) = \sum^n_{i = 1} \lambda_i    
\end{equation}

\begin{equation}
    \text{det}(A) = \prod^n_{i = 1} \lambda_i
\end{equation}

Because there are only 2 variables, there can't be very big changes across more than 2 main directions, so it is possible to quantify the main directions which will be the eigenvectors directions.

\bb{The eigenvalues of the Hessian Matrix are also called principal curvatures and the eigenvectors the principal directions.}












\subsubsection{Analytical Expression for the Coefficientes}

From the equation presented in the end of \ref{sec:calcBetas}, we can re-write the SSE and the normal equations in the following way. 


\quickimage{AIML/AA-cap1-003.png}{0.6}

And arrive at the analytical expression through the simple inversion of the normal equations. Another way of reaching the analytical expression is deriving the cost function.
\quickimage{AIML/AA-cap1-004.png}{0.6}
\quickimage{AIML/AA-cap1-006.png}{0.6}


Note however that:
\quickimage{AIML/AA-cap1-005.png}{0.6}


A final remark on multiple outputs: in case our feature vector serves to estimate more than one output, we can simply use it separately for each output!


\subsubsection{Regularization}

This is the method of taking importance away from the minimization of the errors between the training set supposed outcomes and the actual model outcomes for those samples. If we don't take importance away, the model may become too good at predicting training examples and may forget that it should predict a tendency and generalize well for the test data.

\quickimage{AIML/overfitting.png}{0.3}

Performing a regularization consists on nothing more than adding a new parameter to the cost function, in order to shift away the focus of minimizing the SSE.

There are generally two terms that can be added. One with the \bb{norm of the coefficients squared} and the other is the with the module of the coefficients squared.

For the norm squared, if the regularization is applied to a regression - \bb{which is not a necessity since regularization can even be applied to Neural Networks} - it's called Ridge Regression:
\quickimage{AIML/AA-cap1-007.png}{0.6}


\quickimage{AIML/AA-cap1-008.png}{0.6}

Two key things to note:

\begin{itemize}
    \item Note that $\beta_0$ is usually not included as the data should be normalised already for much better results. Normalised data means data that has zero mean in every feature and outcome.
    \item If $\left(X^TX\right)$ is singular, the least squares estimate is not unique. Regularization will help finding an estimate even then because $\left(X^TX + \lambda I\right)$ is always non-singular.
\end{itemize}

For the simple norm of the coefficients, when applied to a regression problem it is called the Lasso Regression:

\quickimage{AIML/AA-cap1-009.png}{0.6}

\begin{equation}
    ||\beta||_1 = \sum_{j=1}^{n} |\beta_j|
\end{equation}

The key difference between the two is that Ridge aims to minimize the norm of all of them while Lasso aims to minimize the sum of module of each of them. Therefore, Ridge it is much likely to pull closer to zero the biggest ones as those are the ones that matter the most for the Euclidean norm, while Lasso will try to pull each of them to 0, there's a direct dependency between a coefficients and the cost function. This is also why the Lasso Regression is called to do feature selection: because if the SSE doesn't depend on the coefficients, the regularization term with the sum of the norms will put that coefficients to zero very quickly.

\quickimage{AIML/AA-cap1-010.png}{0.4}



Again, recall that the data should be centered - have zero mean - and that after calculating the model we need to de-centre it to obtain the real predictions!
\quickimage{AIML/AA-cap1-011.png}{0.5}

\subsubsection{Optimization problems - Gradient Descent and Newton's Method}

The gradient descent is probably the most know method to approximate a functions minimum. By changing the direction of the step we have the gradient ascent. 

\quickimage{AIML/AA-cap1-012.png}{0.6}

The above expression works because the gradient points the direction of the maximum growth of the function. Therefore, taking a step in the opposite direction will lead to the minimum.

\subsubsection*{Momentum and Adaptive StepSize}
Momentum $0 \leq \alpha \leq 1$:
\quickimage{AIML/AA-cap1-013.png}{0.6}


Note that we want to pick the $\mathbf{x}$ that brings the function to a minimum. Therefore, the ``exponent'' will simply refer to the iteration number, not the sample like in the previous section.
If $\alpha$ is closer to 1 the memory of the previous increment is more taken into account, meaning that the increment will change only slightly. The closer the parameter gets to 0, the closer we get to the normal situation. This is called the momentum term because it gives the convergence some inercia, the behaviour of momentum. By changing the increment slowly, it may converge faster and have less abrupt changes.


Adaptive Step size, with typical values: $u = 1.2, d = 0.8$:

\quickimage{AIML/AA-cap1-014.png}{0.6}

Let f be the function where the objective minimum lies, the Divergence criterium is given by:
\begin{equation}
    f(x^{(n+1)}) > f(x^{(n)})
\end{equation}
Where the threshold is found in the equality.

\subsubsection*{Newton's method}

Given by:
\quickimage{AIML/AA-cap1-015.png}{0.4}


Where the gradient and the Hessian Matrix are given by:

\quickimage{AIML/AA-cap1-016.png}{0.4}

The Newton's method converges insanely fast! But requires the inversion of the Hessian matrix which can be a serious problem...


\subsubsection{Newton's Method - Intuition and Demonstration}
This is a very very good proof!

%do. it. It is in the notebook...


\subsubsection{How to optimise hyperparameters}
There will always be parameters to optimise in order to obtain the best model possible.
It was said before that 3 sets should be selected: training, validation and testing.
And that it was in the validation set that all hyperparameter tunning should be done.
The simplest way is calculate the model with all combinations of the parameters possible and see which one performs better in the validation set. 

There can be one other problem: little data. If the data is too little, dividing it into sets can start to give biased results to the accuracy.

One way of calculating the accuracy with more ... accuracy \dots is splitting the data in k folds and rotate:

\quickimage{AIML/AA-cap1-018.png}{0.4}



Finally, if both things need to be done at the same time, then:
1- k folds need to be made. 
2- for each fold, all values of the hyperparameters need to be used for training and tested in the test set.
Note: it will be used for training T except ($T_i$ and $T_j$) that are, respectively, the test set and the validation set. So the hyperparameters testing will be done with $T_i$.
When the all combinations are done, the hyperparameters are selected for the bet one and the actual model is trained with T except $T_i$ depending on the fold considered.
3- use the performances of each fold to get the best average of performance.

(Note that this is not very used...)
\quickimage{AIML/AA-cap1-019.png}{0.4}








\subsubsection{Neural Networks}


\subsubsection*{Some history}
\quickimage{AIML/AA-cap1-018.png}{0.4}

\quickimage{AIML/AA-cap1-023.png}{0.4}

However, there's a big problem with only being able to distinguish data that can be separate with an hyperplane: very often the data doesn't behave that way.

\quickimage{AIML/AA-cap1-024.png}{0.4}

One can make a more complex analysis of the situation...



\subsubsection*{Activation Functions and Architecture}

Some activation functions: 

\quickimage{AIML/AA-cap1-021.png}{0.4}

Two other are Rectifier Linear Unit and the softmax:
\quickimage{AIML/AA-cap1-022.png}{0.4}

\quickimage{AIML/softmax.png}{0.2}


In the all layers with the exception of the last one, ReLu or the logistic function work very well. The last layer must have a function that returns results between 0 and 1, therefore only SoftMax and sigmoid functions like the logistic function would work properly.





Training methods:
\quickimage{AIML/AA-cap1-025.png}{0.4}



One very interesting fact is that the image features are not selected by anyone. The network itself crafts its features throught backpropagating the changes required to get the images right. This is true for all Neural Networks.

Also, in image recognition for instance, but in deep neural networks in general, the last layers usually are fully connected.

\subsubsection{Neural Networks - BackPropagation}

Some of the most useful websites to check while trying to demonstrate the backpropagation algorithm:
\begin{itemize}
    \item \href{https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60}{\ul{all backpropagation derivatives}}
    \item \href{https://stats.stackexchange.com/questions/94387/how-to-derive-errors-in-neural-network-with-the-backpropagation-algorithm}{\ul{Error (deltas) derivation for backpropagation in neural networks}}
    \item \href{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{3Blue1Brown Neural Networks} - Specially the last one, on backpropagation.
\end{itemize}

The fastest way is the following:

%do. it.


Note that multiple training modes are possible. The normal one is on-line, where the increment to the weight is nothing more than the step multiplied by the respective partial derivative - Equation \eqref{eq1:online}. Then one can do the batch-mode where all the samples are considered for the weights update - Equation \eqref{eq2:batch}. Finally, there's the mini-batch mode that doesn't use all the training samples.

\begin{align}
    w_{ij}^{(l)} &= w_{ij}^{(l)} + \bigtriangleup w_{ij}^{(l)} \\
    \bigtriangleup w_{ij}^{(l)} &= -\eta \frac{\delta C^k}{\delta w_{ij}^{(l)}} \label{eq1:online} \\
    \bigtriangleup w_{ij}^{(l)} &= -\frac{\eta}{m} \sum_{k=1}^m  \frac{\delta C^k}{\delta w_{ij}^{(l)}} \label{eq2:batch}
\end{align}



\subsubsection{Neural Networks - Convolutional} 

Convolutional Neural Networks are extremely useful for applications like image recognition. They take a width x height x image depth 3D array and convolve it with a kernel, generating an 2D array. 

Because many elements in an image are translation invariant, considering patches of the image is one of the best ways of acquiring features.

Convolutional neural networks allow us to use less inputs to our neural networks. We map a region of a picture to just one pixel/input. We tend to use many different kernels - a set of weights to multiply to each of pixel in the set we chose - to perform this step.


\quickimage{AIML/AA-cnn.png}{0.6}


At the end of the convolution, an activation function (like ReLu) is applied.

Note that this convolutional layer can be applied to a 3D array to reduce it to 2D. Moreover, many different kernels can be convolved with the section of the 3D array creating several layers. If there are 20 kernels, we'll have 20 2D arrays, therefore a new 3D array that is all the pixels in the several vicinities, weighted with different kernels.

\quickimage{AIML/AA-cnn3.png}{0.6}




\subsubsection*{Pooling}
2D to 2D but with smaller dimensions.
Downsize the feature array by choosing what we consider to be the most important values. For instance, for each 4x4 square of pixels, we choose the maximum of them (the highest value in grey scale, for instance). Therefore, the end result will have 16x less pixels/inputs. 

\quickimage{AIML/AA-cnn2.png}{0.6}


Overall, the tendency is the kernel to be smaller, but the network to have many many layers.


\subsubsection{Kernels}
Why kernels are important? They facilitate a lot the mapping of features to higher dimensions!

\href{https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d}{\ul{Why kernels?}}



\subsection{Unsupervised Learning}

\subsubsection{Lagrange Multipliers}

The Lagrange Multipliers is a mathematical optimization method, allows the finding of local maxima or minima of a function, subject to equality constraints.

Perfect Explanation why it works:

\href{https://www.quora.com/Why-does-the-method-of-Lagrange-multipliers-work-for-optimization-in-multivariable-calculus-Why-exactly-given-a-function-f-x-y-and-a-constraint-g-x-y-c-can-we-set-the-gradients-of-the-functions-to-be-multiples-of-each-other}{\ul{Quora - Intuition on Langrange Multipliers}}

And a video showing exactly how it's done:

\href{https://www.youtube.com/watch?v=yuqB-d5MjZA}{\ul{Khan Academy - Lagrange Multipliers Example}}



\subsection{Reinforcement Learning and Decision Making}


An agent to make the wisest decision in its situation has to have knowledge on what state he is in, how the environment will evolve, how it will be like if he performs a certain action (taking into account stochastic environments) and a utility function to  know how much that state contributes to its happiness.


